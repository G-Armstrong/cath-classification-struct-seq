{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e65b1ae8-9780-4a68-a49f-0c42dc88829a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'h5py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Suppress element warning - pdb_share files do not have element information, that's ok!\u001b[39;00m\n\u001b[1;32m     15\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElement information is missing, elements attribute will not be populated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'h5py'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import glob\n",
    "import MDAnalysis as mda\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import shutil\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress element warning - pdb_share files do not have element information, that's ok!\n",
    "warnings.filterwarnings(\"ignore\", message=\"Element information is missing, elements attribute will not be populated.\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a65d69-2d60-44de-9589-9fa6d51d97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('cleaned_data.csv')\n",
    "old_df = pd.read_csv('../data/cath_w_seqs_share.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f572b68-d7cc-4c2d-9bfd-053ea5f64cde",
   "metadata": {},
   "source": [
    "### Atomic Resolution Molecualr Graph Features\n",
    "\n",
    "**Node Features**\n",
    "- x, y, z atomic coorindates\n",
    "- atom type, 1 hot encoded for [N, CA, C, O, C_alt] where C_alt is a sidechain carbon\n",
    "\n",
    "**Edge Features**\n",
    "- mean bond angle for all bonds in the local proxmity of each atom\n",
    "- bond distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69075bf-b452-4b14-b782-531cf3361b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate the bond length\n",
    "def bond_length(atom1, atom2):\n",
    "    return np.linalg.norm(atom1.position - atom2.position)\n",
    "\n",
    "# Create a function to calculate the bond angle\n",
    "def bond_angle(atom1, atom2, atom3):\n",
    "    vec1 = atom1.position - atom2.position\n",
    "    vec2 = atom3.position - atom2.position\n",
    "    cosine_angle = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    angle = np.arccos(cosine_angle)\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def recalculate_distance(x1, y1, z1, x2, y2, z2):\n",
    "    point1 = np.array([x1, y1, z1])\n",
    "    point2 = np.array([x2, y2, z2])\n",
    "    distance = np.linalg.norm(point2 - point1)\n",
    "    return distance\n",
    "\n",
    "def remove_hydrogen_atoms(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            # Check if the line is an ATOM record and not a hydrogen atom\n",
    "            if line.startswith('ATOM') and line.split()[11]!= 'H': # Last character will be element symbol\n",
    "                outfile.write(line)\n",
    "            \n",
    "            # Write non-ATOM lines as they are\n",
    "            elif not line.startswith('ATOM'):\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5d6d8b-1ff9-4f33-9da5-14df5b404f51",
   "metadata": {},
   "source": [
    "### Select all modified_pdbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc8104-d27f-4d92-9e28-589abc61c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather AlphaFold2 models - Current state is relaxed. AMBER is used to perform energy minimization\n",
    "alpha_fold_models = glob.glob('../data/*/pdb/*_alphafold_remodeled_relaxed.pdb')\n",
    "\n",
    "# Deprotonate AlphaFold2 models for consistency. In the long run, this is not ideal, and protonated structures would allow hbond modeling\n",
    "deprotonated_models = []\n",
    "for model in alpha_fold_models:\n",
    "    # Create the new filename\n",
    "    dir_name, file_name = os.path.split(model)\n",
    "    base_name, ext = os.path.splitext(file_name)\n",
    "    new_file_name = f\"{base_name}_deprotonated{ext}\"\n",
    "    output_file = os.path.join(dir_name, new_file_name)\n",
    "\n",
    "    # Process the file\n",
    "    remove_hydrogen_atoms(model, output_file)\n",
    "    deprotonated_models.append(output_file)\n",
    "\n",
    "print(deprotonated_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3215e8-cb0f-4b0d-a8be-c11953d5c176",
   "metadata": {},
   "source": [
    "### Select all original_pdbs where the new_seq is identical to the old_seq\n",
    "- These pdbs do not contain gap residues and did not have to be remodeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf176c-4890-47c3-8a0b-d31658cce4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_seq_change_models = []\n",
    "for idx, row in df.iterrows():\n",
    "    cath_id = row['cath_id']\n",
    "    new_seq = row['sequences']\n",
    "    old_seq = old_df.loc[old_df['cath_id'] == cath_id , 'sequences'].values[0]\n",
    "    if new_seq == old_seq:\n",
    "        # Find the original file\n",
    "        original_file_path = glob.glob(f'../data/{cath_id}/pdb/{cath_id}')[0]\n",
    "        \n",
    "        # Define the new file path with .pdb extension\n",
    "        new_file_path = os.path.join(os.path.dirname(original_file_path), f\"{cath_id}.pdb\")\n",
    "              \n",
    "        # Copy the file, overwriting if it already exists\n",
    "        shutil.copy2(original_file_path, new_file_path)\n",
    "        \n",
    "        # Add the new file path to the list\n",
    "        no_seq_change_models.append(new_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22fd728-9d68-4b0e-9afe-58d3c4542a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cath_id'] == '2w3sB01']['sequences'].item() == old_df[old_df['cath_id'] == '2w3sB01']['sequences'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37221026-d447-4179-8dd9-e5db0fc12732",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_seq_change_models[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dafff2-caf4-4d51-831b-9225fd6bc364",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = alpha_fold_models + no_seq_change_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651df81b-5430-4ca8-bc04-89278b870b84",
   "metadata": {},
   "source": [
    "### Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae6177-8854-42e2-abfd-ab8c0872a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list and the character to one-hot encode\n",
    "atom_types = ['N', 'CA', 'C', 'O', 'C_alt'] # 'C_alt' covers sidechain Carbon atoms\n",
    "\n",
    "data_list = [] # This is where the final PyTorch Geometric graphs will be stored\n",
    "\n",
    "with open('graph_dataset_output.txt', 'w') as output_file: # Open a file for all output to reduce I/O burden\n",
    "    for struct in tqdm(all_files):\n",
    "        cath_id = struct.split(\"/\")[2]\n",
    "    \n",
    "        # NetworkX Graph \n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Create a Universe Object with guess_bonds enabled \n",
    "        u = mda.Universe(struct, guess_bonds=True)\n",
    "    \n",
    "        print(f'[TOPOLOGY] Setting node features for CATH_ID {cath_id}', file=output_file)\n",
    "        # Iterate over all residues and atoms in the universe\n",
    "        for residue in u.residues:\n",
    "            # Loop through each atom in the residue\n",
    "            for atom in residue.atoms:\n",
    "                # Create the nodel label from its attributes\n",
    "                residue_name = residue.resname\n",
    "                residue_id = residue.resid\n",
    "                chain_id = residue.segid\n",
    "                atom_name = atom.name\n",
    "                label = f'{residue_name}{residue_id}.{chain_id}.{atom_name}'\n",
    "    \n",
    "                # Determine features for this atomic node\n",
    "                element = atom_name[0]\n",
    "                if element == 'C':\n",
    "                    if atom_name == 'CA':\n",
    "                        element = 'CA'\n",
    "                    elif atom_name != 'C':\n",
    "                        element = 'C_alt'     \n",
    "                        \n",
    "                one_hot_element = [1 if atom == element else 0 for atom in atom_types]\n",
    "    \n",
    "                node_attributes = {\n",
    "                                    'element' : one_hot_element,\n",
    "                                    'x' : str(atom.position[0]),\n",
    "                                    'y' : str(atom.position[1]),\n",
    "                                    'z' : str(atom.position[2])}\n",
    "    \n",
    "                # Add node\n",
    "                G.add_node(label, **node_attributes)\n",
    "                \n",
    "        print(f'[TOPOLOGY] Setting edge features for CATH_ID {cath_id}',file=output_file)\n",
    "        for bond in u.bonds:\n",
    "            # Parse out interacting atoms in this bond\n",
    "            atom1, atom2 = bond\n",
    "    \n",
    "            # Determine edge features\n",
    "            '''1) Distance'''\n",
    "            dist = bond_length(atom1, atom2)\n",
    "            \n",
    "            '''2) Mean bond angle'''\n",
    "            bond_angles = []\n",
    "    \n",
    "            neighbors_atom2 = atom2.bonded_atoms\n",
    "            for atom3 in neighbors_atom2:\n",
    "                if atom3 != atom1:\n",
    "                    angle_deg = bond_angle(atom1, atom2, atom3)\n",
    "                    bond_angles.append(angle_deg)\n",
    "        \n",
    "            # switch roles to include all angles relevant to the connection\n",
    "            temp = atom1\n",
    "            atom1 = atom2\n",
    "            atom2 = temp\n",
    "        \n",
    "            # and append new calculations...\n",
    "            neighbors_atom2 = atom2.bonded_atoms\n",
    "            for atom3 in neighbors_atom2:\n",
    "                if atom3 != atom1:\n",
    "                    angle_deg = bond_angle(atom1, atom2, atom3)\n",
    "                    bond_angles.append(angle_deg)\n",
    "        \n",
    "            mean_bond_angle = np.mean(bond_angles)\n",
    "    \n",
    "            edge_attributes = {\n",
    "                        'distance' : str(dist),\n",
    "                        'bond_angle' : str(mean_bond_angle)}\n",
    "    \n",
    "            # Convet `atom1` and `atom2` back to original identities \n",
    "            atom1, atom2 = bond\n",
    "    \n",
    "            # Create atom labels compatible with nodes in the graph\n",
    "            atom1_label = f'{atom1.resname}{atom1.resid}.{atom1.segid}.{atom1.name}'\n",
    "            atom2_label = f'{atom2.resname}{atom2.resid}.{atom2.segid}.{atom2.name}'\n",
    "    \n",
    "            # Add edges, excluding self-connections\n",
    "            G.add_edge(atom1_label, atom2_label, **edge_attributes)\n",
    "    \n",
    "        '''\n",
    "        Save NetworkX graph with untransformed features to ensure human interpretability \n",
    "        '''\n",
    "        print(f'[Checkpoint] Saving NetworkX graph for CATH_ID {cath_id}', file=output_file)\n",
    "        gml_fileName = f'../data/{cath_id}/networkx/{cath_id}_graph.gml'\n",
    "        pkl_fileName = f'../data/{cath_id}/networkx/{cath_id}_graph.pkl' # Faster I/O for .pkl\n",
    "        nx.write_gml(G, gml_fileName)\n",
    "        with open(pkl_fileName, 'wb') as f:\n",
    "            pickle.dump(G, f)\n",
    "    \n",
    "        #####################GRAPH LEVEL OPERATIONS#############################\n",
    "        \n",
    "        print(f'[SE3] Transforming geometric feature values for CATH_ID {cath_id}', file=output_file)\n",
    "        # Collect Coordinates\n",
    "        coordinates = [[float(attributes['x']), float(attributes['y']), float(attributes['z'])] for _, attributes in G.nodes(data=True)]\n",
    "    \n",
    "        '''\n",
    "        Rotation Invariance of Atomic Coordinates\n",
    "    \n",
    "        Each molecular structure is aligned to its own principal axes through PCA, \n",
    "        generating a rotation matrix that optimally aligns it to a standard\n",
    "        reference frame\n",
    "        '''\n",
    "        # Step 1: Compute the principal axes (eigenvectors) that represent the directions of maximum variance in the distribution of atoms.\n",
    "        pca = PCA(n_components=3) # X, Y, Z dimensions\n",
    "        pca.fit(coordinates)\n",
    "        \n",
    "        # Obtain rotation matrix to align with principal axes\n",
    "        rotation_matrix = pca.components_.T  # Ensure it's a rotation matrix by ensuring determinant is 1 or -1\n",
    "        \n",
    "        # Step 2: Apply rotation matrix to align coordinates to a cannonical orientation\n",
    "        transformed_coordinates = np.dot(coordinates, rotation_matrix)\n",
    "    \n",
    "        '''\n",
    "        Translation Invariance of Atomic Coordinates\n",
    "    \n",
    "        Enhances the robustness of the model to arbitrary positional variations, \n",
    "        which otherwise describe the same structure\n",
    "        '''\n",
    "        # Calculate centroid (mean) of coordinates\n",
    "        centroid = np.mean(transformed_coordinates, axis=0)\n",
    "        \n",
    "        # Center coordinates about origin\n",
    "        centered_coordinates = transformed_coordinates  - centroid\n",
    "        \n",
    "        # Initialize MinMaxScaler with feature_range set to [-1, 1]\n",
    "        scaler_coordinates = MinMaxScaler(feature_range=(-1, 1))\n",
    "        \n",
    "        # Fit the scaler to the centered atomic coordinates\n",
    "        scaler_coordinates.fit(centered_coordinates)\n",
    "        \n",
    "        # Normalize the atomic coordinates\n",
    "        normalized_coordinates = scaler_coordinates.transform(centered_coordinates)\n",
    "    \n",
    "        '''\n",
    "        Normalize bond angles to from [0, 180] to [0, 1]\n",
    "        '''\n",
    "        # Collect bond angles\n",
    "        bond_angles = [[float(attributes['bond_angle'])] for _, _, attributes in G.edges.data()]\n",
    "    \n",
    "        # Initialize MinMaxScaler\n",
    "        scaler_bond_angles = MinMaxScaler()\n",
    "    \n",
    "        # Fit the scaler on the data and transform bond_angles to [0, 1]\n",
    "        normalized_angles = scaler_bond_angles.fit_transform(bond_angles)\n",
    "    \n",
    "        '''\n",
    "        Convert to PyTorch Geometric Object\n",
    "        '''\n",
    "        print(f'[PyTorch] Converting NetworkX graph for CATH_ID {cath_id} to PyTorch Geometric\\n', file=output_file)\n",
    "    \n",
    "        sequence = df[df['cath_id'] ==  cath_id]['sequences'].item()\n",
    "        target = int(df[df['cath_id'] == cath_id]['label'].item())\n",
    "        node_labels = list(G.nodes)\n",
    "    \n",
    "        # Extract node features\n",
    "        node_features = []\n",
    "        for j, (_, attributes) in enumerate(G.nodes(data=True)):\n",
    "            one_hot_element = attributes['element']\n",
    "            feature = [\n",
    "                *one_hot_element, # Unpack the 1-hot encoded list\n",
    "                # Geometric features rotated, translated, and scaled to [-1, 1]\n",
    "                normalized_coordinates[j][0], # X\n",
    "                normalized_coordinates[j][1], # Y\n",
    "                normalized_coordinates[j][2]  # Z\n",
    "                ]\n",
    "            node_features.append(feature)\n",
    "    \n",
    "        # Extract edge features\n",
    "        edge_features = []\n",
    "        edge_index = []\n",
    "        for k, (u, v, attributes) in enumerate(G.edges.data()):\n",
    "    \n",
    "            # Recalculate bond distances for the [-1, 1] scaled coordinate space\n",
    "            u_id = node_labels.index(u) # Node ID for the `u` atom in the bond\n",
    "            v_id = node_labels.index(v) # Node ID for the `v` atom in the bond\n",
    "            # Use the node indices to access the respective normalized coordinates\n",
    "            x1, y1, z1 = normalized_coordinates[u_id][0], normalized_coordinates[u_id][1], normalized_coordinates[u_id][2]\n",
    "            x2, y2, z2 = normalized_coordinates[v_id][0], normalized_coordinates[v_id][1], normalized_coordinates[v_id][2]\n",
    "            \n",
    "            feature = [\n",
    "                recalculate_distance(x1, y1, z1, x2, y2, z2),\n",
    "                normalized_angles[k][0]\n",
    "                ]\n",
    "            edge_features.append(feature)\n",
    "    \n",
    "        # Data Object Instantiation\n",
    "        data = Data(\n",
    "                    cath_id = [cath_id],\n",
    "                    seq = [sequence],\n",
    "                    node_labels = node_labels,\n",
    "                    node_feat = torch.Tensor(node_features),\n",
    "                    edge_feat = torch.Tensor(edge_features),\n",
    "                    adj_matrix = torch.Tensor(nx.to_numpy_array(G)),   # Dense Adjacency Matrix\n",
    "                    target = torch.tensor(target)                      # Target class for this cath_id\n",
    "                    )\n",
    "    \n",
    "        # Append the completed PyTorch Geometric Object\n",
    "        data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055ae57-8834-4dc3-bfab-4fde14b57aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a3e651-ef6d-4e1e-8198-53600dae5c74",
   "metadata": {},
   "source": [
    "## Stratified Data Split\n",
    "- Group the data by superfamily, keeping all proteins from the same superfamily together to prevent splitting related proteins across the train and test sets.\n",
    "- Perform a stratified split to maintain the proportion of each superfamily in both the training and testing sets, ensuring a representative distribution.\n",
    "- Within each superfamily, randomly select proteins to assign to the training and testing sets.\n",
    "- Use a common split ratio, such as 80% for training and 20% for testing\n",
    "- Ensure that small superfamilies, with very few members, are represented in both sets if possible. We want both our training and testing sets to be representative of the overall dataset. Including small superfamilies in both sets helps maintain this representation. If we only include large superfamilies in the test set, we might overfit to common superfamilies and misjudge model performance on more rare superfamilies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc28f2-6167-4aee-bdad-f51a0cf876e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "superfamily_sizes = df['superfamily'].value_counts()\n",
    "superfamily_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f4be4-f6d5-42e4-95ca-e5602a6a6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split_by_superfamily(df, test_size=0.2, small_family_threshold=4, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Group by superfamily and get sizes\n",
    "    superfamily_sizes = df['superfamily'].value_counts()\n",
    "    \n",
    "    # Separate small and large superfamilies\n",
    "    small_families = superfamily_sizes[superfamily_sizes < small_family_threshold].index.tolist() \n",
    "    large_families = superfamily_sizes[superfamily_sizes >= small_family_threshold].index.tolist()\n",
    "    \n",
    "    # Shuffle both lists\n",
    "    np.random.shuffle(small_families)\n",
    "    np.random.shuffle(large_families)\n",
    "    \n",
    "    # Split large families\n",
    "    split_point = int(len(large_families) * (1 - test_size))\n",
    "    train_large_names = large_families[:split_point] # 80% train\n",
    "    test_large_names = large_families[split_point:]  # 20% test\n",
    "\n",
    "    # Initialize train and test dataframes with large family data\n",
    "    train_df = df[df['superfamily'].isin(train_large_names)]\n",
    "    test_df = df[df['superfamily'].isin(test_large_names)]\n",
    "    \n",
    "    # Handle small families\n",
    "    for family in small_families:\n",
    "        family_data = df[df['superfamily'] == family]\n",
    "        if len(family_data) == 1:\n",
    "            # If only one instance, always put it in the training set\n",
    "            train_df = pd.concat([train_df, family_data])\n",
    "        else:\n",
    "            family_train, family_test = train_test_split(family_data, test_size=test_size, random_state=random_state)\n",
    "            train_df = pd.concat([train_df, family_train])\n",
    "            test_df = pd.concat([test_df, family_test])\n",
    "\n",
    "    print(f\"Train set size: {len(train_df)}\")\n",
    "    print(f\"Test set size: {len(test_df)}\")\n",
    "    print(f\"Actual test size: {len(test_df) / len(df):.2f}\")\n",
    "    print(f\"Small families in train: {sum(train_df['superfamily'].isin(small_families))}\")\n",
    "    print(f\"Small families in test: {sum(test_df['superfamily'].isin(small_families))}\")\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f7d9f2-c405-4e45-a663-029285819836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "train_df, test_df = stratified_split_by_superfamily(df)\n",
    "\n",
    "# Save the split datasets\n",
    "train_df.to_csv('../models/train_data.csv', index=False)\n",
    "test_df.to_csv('../models/test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86c2587-59e8-434b-9a68-4ea2a1546c40",
   "metadata": {},
   "source": [
    "## Saving the PyG Objects Allows Preprocessing to be Performed Only Once\n",
    "- Afterwards, you simply reload data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8189ea-02ae-4493-b814-ca563d073cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition PyTorch Geometric Objects by train_df and test_df and Save\n",
    "train_cath_ids = list(train_df['cath_id'])\n",
    "test_cath_ids = list(train_df['cath_id'])\n",
    "\n",
    "train_file = '../models/train_compressed.pt'\n",
    "test_file = '../models/test_compressed.pt'\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for pyg_obj in data_list:\n",
    "    # Looks like: Data(cath_id=[1], seq=[1], node_labels=[3731], node_feat=[3731, 8], edge_feat=[3762, 2], adj_matrix=[3731, 3731], target=6)\n",
    "    obj_dict = {\n",
    "            'cath_id': pyg_obj.cath_id,\n",
    "            'seq': pyg_obj.seq,\n",
    "            'node_labels': pyg_obj.node_labels,\n",
    "            'node_feat': pyg_obj.node_feat,\n",
    "            'edge_feat': pyg_obj.edge_feat,\n",
    "            'adj_matrix': pyg_obj.adj_matrix,\n",
    "            'target': pyg_obj.target\n",
    "        }\n",
    "\n",
    "    if pyg_obj.cath_id[0] in train_cath_ids:\n",
    "        train_data.append(obj_dict)\n",
    "        \n",
    "    elif pyg_obj.cath_id[0] in test_cath_ids:\n",
    "        test_data.append(obj_dict)\n",
    "\n",
    "# Save\n",
    "torch.save(train_data, train_file, _use_new_zipfile_serialization=True)\n",
    "torch.save(test_data, test_file, _use_new_zipfile_serialization=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43183357-97a1-46ec-bfbc-bec3bd203050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pyg_list(file_path):\n",
    "    loaded_list = torch.load(file_path)\n",
    "    data_list = []\n",
    "    for data_dict in loaded_list:\n",
    "        data = Data(\n",
    "            cath_id=data_dict['cath_id'],\n",
    "            seq=data_dict['seq'],\n",
    "            node_labels=data_dict['node_labels'],\n",
    "            node_feat=data_dict['node_feat'],\n",
    "            edge_feat=data_dict['edge_feat'],\n",
    "            adj_matrix=data_dict['adj_matrix'],\n",
    "            target=data_dict['target']\n",
    "        )\n",
    "        data_list.append(data)\n",
    "    \n",
    "    print(f\"Loaded {len(data_list)} PyG objects from {file_path}\")\n",
    "    return data_list\n",
    "\n",
    "test_list = load_pyg_list('../models/train_compressed.pt')\n",
    "test_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
